
<h1 align="center">ğŸ™ï¸ Emotion Detection from Raw Audio Waveform</h1>

<p align="center">
  <b>Detect human emotions from raw .wav files using deep learning and pretrained speech models.</b><br>
  Built using Hugging Face Transformers, PyTorch, and Torchaudio.
</p>

---

## ğŸš€ Overview

This project classifies human emotions like **Happy**, **Sad**, **Angry**, and **Neutral** directly from raw audio (`.wav`) files. It utilizes a Hugging Face pretrained model fine-tuned on the **RAVDESS** emotional speech dataset. The entire processâ€”from preprocessing to training and inferenceâ€”can be run using simple Python scripts.

---
## ğŸ“š Table of Contents

- [ğŸš€ Overview](#-overview)
- [ğŸ§  How the Model Works](#-how-the-model-works)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ› ï¸ Installation](#ï¸-installation)

### ğŸ¯ Recreate the Model (Step-by-Step)
- [Step 1: Preprocess the Dataset](#-step-1-preprocess-the-dataset)
- [Step 2: Train the Classifier](#-step-2-train-the-classifier)
- [Step 3: Run Inference](#-step-3-run-inference)

### ğŸ§ª Sample Output
- [ğŸ“Š Classification Report](#1-classification-report)
- [ğŸ” Prediction Output](#2-prediction-output)
- [ğŸ“‰ Confusion Matrix](#3-confusion-matrix)
- [ğŸ“ˆ Wave-Output](#4-wave-output)

- [ğŸ” Model Details](#-model-details)
- [ğŸŒŸ Features](#-features)
- [ğŸ“¦ Requirements](#-requirements)
- [ğŸ’¡ Future Ideas](#-future-ideas)
- [ğŸ‘¨â€ğŸ’» Author](#-author)

---
## ğŸ§  How the Model Works

1. **Raw Audio Input**  
   You input a mono `.wav` file recorded at any sample rate.

2. **Preprocessing**  
   The audio is normalized and resampled to 16kHz using `torchaudio`.

3. **Feature Extraction**  
   The pretrained **HuBERT** model (from Hugging Face) extracts deep audio embeddings.

4. **Classifier Head**  
   A dense neural network is trained on top of these embeddings using labeled emotion data (RAVDESS).

5. **Prediction**  
   The model outputs the most probable emotion class for the given voice input.

---

## ğŸ“ Project Structure

```
Emotion_detection_by_wave_formate/
â”œâ”€â”€ audio_waveform_viewer.py     # Displays waveform of audio files
â”œâ”€â”€ preprocess_ravdess.py        # Prepares RAVDESS dataset
â”œâ”€â”€ train_emotion_model.py       # Trains the classifier on HuBERT embeddings
â”œâ”€â”€ predict_emotion.py           # Predicts emotion for new audio input
â”œâ”€â”€ model/
â”‚   â””â”€â”€ final_emotion_model/     # Trained model is saved here
â”œâ”€â”€ dataset/                     # Raw and processed audio files
â”œâ”€â”€ sample_output/               # Store screenshots and predictions
â”œâ”€â”€ requirements.txt             # All required dependencies
â””â”€â”€ README.md                    # You are here!
```

---

## ğŸ› ï¸ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/dhanesh-j/Emotion_detection_by_wave_formate.git
cd Emotion_detection_by_wave_formate
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ¯ Recreate the Model (Step-by-Step)

### ğŸ”¹ Step 1: Preprocess the Dataset

Ensure RAVDESS dataset is available under `dataset/RAVDESS/`

```bash
python preprocess_ravdess.py --input_dir dataset/RAVDESS/ --output_dir dataset/processed/ --sample_rate 16000
```

### ğŸ”¹ Step 2: Train the Classifier

```bash
python train_emotion_model.py   --data_dir dataset/processed/   --pretrained_model superb/hubert-large-superb-er   --output_dir model/final_emotion_model/
```

### ğŸ”¹ Step 3: Run Inference

```bash
python predict_emotion.py --model_dir model/final_emotion_model/ --input_audio sample_output/sample.wav
```

---

## ğŸ§ª Sample Output

Hereâ€™s a visual summary of the model's performance across different emotions:

### 1. Classification Report
<img src="sample_output/classification report.png" width="600"/>
The graph shows high precision and recall for emotions like *Sad* and *Neutral*, with consistent accuracy across all categories.

### 2.Prediction Output
<img src="sample_output/prediction output.png"  width = "600"/>
The above image represents the Emotion Detected after analysing the audio in realtime (live recorded audio).

### 3.Confusion Matrix
<img src="sample_output/Confusion_matrix.png" width = "600"/>
The above image represents the confusion matrix of the model

### 4. Wave-Output

<img src="sample_output/RecordedAudioWaveform.jpg" width="600"/>
The above image represents the Realtime Wave-Form of Recorded audio, measured by Time / Seconds.

---

## ğŸ” Model Details

- **Backbone**: [`superb/hubert-large-superb-er`](https://huggingface.co/superb/hubert-large-superb-er)
- **Dataset**: [RAVDESS](https://zenodo.org/record/1188976)
- **Input**: Raw waveform
- **Output**: Emotion class (`happy`, `sad`, `angry`, `neutral`)

---

## ğŸŒŸ Features

- âœ… No speech-to-text required
- âœ… Lightweight training using pretrained embeddings
- âœ… Easily extendable with other datasets
- âœ… Compatible with Gradio or Streamlit for UI

---

## ğŸ“¦ Requirements

- Python 3.8+
- torch
- torchaudio
- transformers
- librosa
- matplotlib

Install them using:

```bash
pip install -r requirements.txt
```

---

## ğŸ’¡ Future Ideas

- ğŸ¤ Real-time microphone support
- ğŸŒ Multilingual emotion detection
- ğŸ“Š Interactive dashboard with waveform + prediction
- ğŸ§ª Confusion matrix and training metrics visualization

---

## ğŸ‘¨â€ğŸ’» Author

**Dhanesh J. & Akash Krishnan**  
Third-year mini project Computer Science student passionate about AI, voice recognition, and applied ML.
